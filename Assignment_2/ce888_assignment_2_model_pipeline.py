# -*- coding: utf-8 -*-
"""CE888 - Assignment 2_model_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fEd4u48PdzcENWgZRlG4glPjuhNuvrLq
"""

#Mounts Google drive where all the images are stored in zip files
from google.colab import drive
drive.mount('/content/gdrive')

#Unzips all the files into google colab
!unzip "/content/gdrive/My Drive/CE888_assignment/Test.zip"
!unzip "/content/gdrive/My Drive/CE888_assignment/Training.zip"

#Imports all the libraries required
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Dropout
from keras.layers import Dense
from keras.applications.inception_resnet_v2 import InceptionResNetV2
from keras.applications import ResNet101
from keras.applications import VGG16
from keras.applications import Xception
from keras.applications.densenet import DenseNet121
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os
!pip install split-folders
import splitfolders

#Splits the training directory using stratification to achieve an 80% training set and 20% validation
splitfolders.ratio("Training", output="output", seed=1337, ratio=(.8, .2), group_prefix=None)
#Links the training set to the new training folder
train_set = 'output/train'
#Links the validation set to the new validation folder
val_set = 'output/val'
#Links the test set to the test folder
test_set = 'Test'

#Prints the amount of images in the training, validation and test files to ensure the split has worked and all images are present
print('train fire images:', len(os.listdir('output/train/Fire')))
print('train no_fire images:', len(os.listdir('output/train/No_Fire')))

print('validation fire images:', len(os.listdir('output/val/Fire')))
print('validation no_fire images:', len(os.listdir('output/val/No_Fire')))

print('test fire images:', len(os.listdir('Test/Fire')))
print('test no_fire images:', len(os.listdir('Test/No_Fire')))

#Batch size to be used throughout the project
batch_size = 32

#Numbers of images in each class for the training set to perform the weighting calculation
Fire_images = 20014
No_fire_images = 11485
Total_images = 31499

#Calculate the weights to solve class imbalance
Fire_weight = (1 / Fire_images)*(Total_images)/2.0 
No_fire_weight = (1 / No_fire_images)*(Total_images)/2.0
class_weight = {0: Fire_weight, 1: No_fire_weight}

#Prints the weights assigned
print('Weight for Fire: {:.2f}'.format(Fire_weight))
print('Weight for No_fire: {:.2f}'.format(No_fire_weight))

#Set early stopping to prevent overfitting
early_stop = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

#Generates the training images and sets the augmentation techniques
train_datagen = ImageDataGenerator(rescale=1./255,
    rotation_range=90,
    horizontal_flip = True,
    vertical_flip = True,
    brightness_range=[0.5, 0.7])

print('Training')
#Pulls the images from the specified training directory and generates batches the augmented images
train_generator = train_datagen.flow_from_directory(
    train_set,
    #Image size kept at 254 by 254 to retain maximum information
    target_size=(254, 254),
    batch_size=batch_size,
    class_mode='binary',
    #Shuffles the training set to avoid influencing the model
    shuffle = True)

#Generates the validation images and rescales them
val_datagen = ImageDataGenerator(rescale=1./255)
print('Validation')
#Pulls the images from the specified validation directory
validation_generator = val_datagen.flow_from_directory(
    val_set,
    #Image size kept at 254 by 254 to retain maximum information
    target_size=(254, 254),
    batch_size=batch_size,
    class_mode='binary',
    shuffle = True)

#Generates the test images and rescales them
test_datagen = ImageDataGenerator(rescale=1./255)
print('Test')
#Pulls the images from the specified test directory
test_generator = test_datagen.flow_from_directory(
    test_set,
    target_size=(254, 254),
    batch_size=batch_size,
    class_mode='binary',
    shuffle = False)

#As all the models in this study have the same structure this function takes the model type as an input (ResNet etc)
#It then adds on the common layers, compiles the loss, optimiser and training metric and returns the full model structure

def model_structure(models):
  model = Sequential()
  #.add(models) takes the initial transfer learning structure passed through the function
  model.add(models)
  model.add(GlobalAveragePooling2D())
  model.add(Dropout(0.5))
  model.add(Dense(1, activation='sigmoid'))
  model.summary()
  model.compile(loss='binary_crossentropy',
              optimizer=Adam(lr=0.00005),
              metrics=['accuracy'])
  return model

#This function takes the fitted model and plots the training/validation loss and accuracy
def model_eval(models):
  #Loss plot
  plt.plot(models.history['loss'])
  plt.plot(models.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'validation'], loc='upper left')
  plt.show()

  #Accuracy plot
  plt.plot(models.history['accuracy'])
  plt.plot(models.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'validation'], loc='upper left')
  plt.show()

#This function takes the full model structure, fits the generator and specifies the training structure
def model_fit(models):
  history = models.fit(
      train_generator,
      epochs = 10,
      #This ensures the number of steps covers all images at batch size 32
      steps_per_epoch = train_generator.samples // 32,
      validation_steps = validation_generator.samples // 32,
      validation_data = validation_generator,
      callbacks = early_stop)
  return history

#This function takes the model after it has been trained and predicts on the test set
#It rounds the predictions from the test set to 0 or 1 for classification
#It defines the test labels needed for evaluation and class_labels are the names e.g. Fire, No Fire
def predict(models):
  predictions = models.predict(test_generator)
  class_predictions = np.where(predictions>=0.5, 1, 0)
  test_labels = test_generator.classes 
  class_labels = list(test_generator.class_indices.keys())
  return class_predictions, test_labels, class_labels

#This function takes the predictions and labels and produces a classsification report and confusion matrix
def eval_metrics(class_predictions, test_labels, class_labels):
  print(classification_report(test_labels, class_predictions, target_names= class_labels))
  conf_matrix = confusion_matrix(test_labels, class_predictions)
  confdisplay = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels = class_labels)
  confdisplay = confdisplay.plot(values_format = '.0f')

#This section of code pulls in the DenseNet structure without the classifier and specifies the input shape
dn = DenseNet121(include_top=False, input_shape=(254,254,3))
#The functions for model structure, fitting and predictions are all called using DenseNet
dense = model_structure(dn)
dense_history = model_fit(dense)
dense_predictions, test_labels, class_labels = predict(dense)

#Takes the predictions and produces the classification report/confusion matrix for DenseNet
eval_metrics(dense_predictions, test_labels, class_labels)

#Takes the history from the training of the DenseNet model and plots the loss/accuracy per epoch
model_eval(dense_history)

#This section of code pulls in the ResNet structure without the classifier and specifies the input shape
rn = ResNet101(
    include_top=False,
    input_shape=(254,254,3))
#The functions for model structure, fitting and predictions are all called using ResNet
res = model_structure(rn)
res_history = model_fit(res)
res_predictions, test_labels, class_labels = predict(res)

#Takes the predictions and produces the classification report/confusion matrix for ResNet
eval_metrics(res_predictions, test_labels, class_labels)

#Takes the history from the training of the ResNet model and plots the loss/accuracy per epoch
model_eval(res_history)

#This section of code pulls in the VGG16 structure without the classifier and specifies the input shape
vgg16 = VGG16(
    include_top=False,
    input_shape=(254,254,3))
#The functions for model structure, fitting and predictions are all called using VGG16
vgg = model_structure(vgg16)
vgg_history = model_fit(vgg)
vgg_predictions, test_labels, class_labels = predict(vgg)

#Takes the predictions and produces the classification report/confusion matrix for VGG16
eval_metrics(vgg_predictions, test_labels, class_labels)

#Takes the history from the training of the VGG16 model and plots the loss/accuracy per epoch
model_eval(vgg_history)

#This section of code pulls in the Xception structure without the classifier and specifies the input shape
xception = Xception(
    include_top=False,
    input_shape=(254,254,3))
#The functions for model structure, fitting and predictions are all called using Xception
xcpt = model_structure(xception)
xcpt_history = model_fit(xcpt)
xcpt_predictions, test_labels, class_labels = predict(xcpt)

#Takes the predictions and produces the classification report/confusion matrix for Xception
eval_metrics(xcpt_predictions, test_labels, class_labels)

#Takes the history from the training of the Xception model and plots the loss/accuracy per epoch
model_eval(xcpt_history)

#This function takes the full model structure, fits the generator and specifies the training structure
#It is the same as the function above but includes the class weights
def weight_model_fit(models):
  history = models.fit(
      train_generator,
      steps_per_epoch = train_generator.samples // 32,
      validation_data = validation_generator, 
      validation_steps = validation_generator.samples // 32,
      epochs = 10,
      class_weight = class_weight,
      callbacks = early_stop)
  return history

#This section of code pulls in the DenseNet structure without the classifier and specifies the input shape
weights_dn = DenseNet121(include_top=False, input_shape=(254,254,3))
#The functions for model structure (including weights), fitting and predictions are all called using DenseNet
dense_weight = model_structure(weights_dn)
dense_weight_history = weight_model_fit(dense_weight)
dense_weight_predictions, test_labels, class_labels = predict(dense_weight)

#Takes the predictions and produces the classification report/confusion matrix for the weighted DenseNet
eval_metrics(dense_weight_predictions, test_labels, class_labels)

#Takes the history from the training of the weighted DenseNet model and plots the loss/accuracy per epoch
model_eval(dense_weight_history)